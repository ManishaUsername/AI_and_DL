{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 6 - Sentiment analysis\n",
        "\n",
        "Dataset name - Sentiment140 dataset with 1.6 million tweets(kaggle)\n",
        "# Dataset: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
        "\n"
      ],
      "metadata": {
        "id": "lRukTIUm2zYl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mN02WC6lG0jl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load Dataset\n",
        "df = pd.read_csv(\"/content/archive (12).zip\",\n",
        "                 encoding=\"latin-1\",\n",
        "                 names=[\"target\",\"ids\",\"date\",\"flag\",\"user\",\"text\"])\n",
        "print(df);\n",
        "\n",
        "# Keep only required columns\n",
        "df = df[['target','text']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSjKId4GIcII",
        "outputId": "3b288029-c967-4c67-a3ef-024bc7d1cef7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         target         ids                          date      flag  \\\n",
            "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
            "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
            "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
            "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "...         ...         ...                           ...       ...   \n",
            "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
            "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
            "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
            "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
            "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
            "\n",
            "                    user                                               text  \n",
            "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
            "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
            "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
            "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
            "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
            "...                  ...                                                ...  \n",
            "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
            "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
            "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
            "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
            "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
            "\n",
            "[1600000 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Clean Dataset\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)        # remove links\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)           # remove mentions\n",
        "    text = re.sub(r\"#\", \"\", text)              # remove hashtags symbol\n",
        "    text = re.sub(r\"[^a-zA-Z']\", \" \", text)    # keep letters only\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "df['text'] = df['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "FWyb0AK7Ik1O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "try:\n",
        "    english_stops = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    english_stops = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    df = pd.read_csv('/content/archive (12).zip',\n",
        "                     encoding=\"latin-1\",\n",
        "                     names=[\"target\",\"ids\",\"date\",\"flag\",\"user\",\"text\"])\n",
        "    x_data = df['text']       # Reviews/Input - Use 'text' column\n",
        "    y_data = df['target']    # Sentiment/Output - Use 'target' column\n",
        "\n",
        "    # PRE-PROCESS REVIEW\n",
        "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
        "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
        "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
        "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
        "\n",
        "    # ENCODE SENTIMENT -> 0 & 1 (Map original 0 and 4 to 0 and 1)\n",
        "    y_data = y_data.replace({0: 0, 4: 1})\n",
        "\n",
        "\n",
        "    return x_data, y_data\n",
        "\n",
        "x_data, y_data = load_dataset()\n",
        "\n",
        "print('Reviews')\n",
        "print(x_data, '\\n')\n",
        "print('Sentiment')\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuFp71jk3exV",
        "outputId": "06114037-a813-4a99-f5a1-0865fb83658e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reviews\n",
            "0          [switchfoot, http, twitpic, com, zl, awww, bum...\n",
            "1          [upset, update, facebook, texting, might, cry,...\n",
            "2          [kenichan, i, dived, many, times, ball, manage...\n",
            "3                    [whole, body, feels, itchy, like, fire]\n",
            "4                   [nationwideclass, behaving, mad, i, see]\n",
            "                                 ...                        \n",
            "1599995    [just, woke, having, school, best, feeling, ever]\n",
            "1599996    [thewdb, com, very, cool, hear, old, walt, int...\n",
            "1599997           [are, ready, mojo, makeover, ask, details]\n",
            "1599998    [happy, th, birthday, boo, alll, time, tupac, ...\n",
            "1599999    [happy, charitytuesday, thenspcc, sparkscharit...\n",
            "Name: text, Length: 1600000, dtype: object \n",
            "\n",
            "Sentiment\n",
            "0          0\n",
            "1          0\n",
            "2          0\n",
            "3          0\n",
            "4          0\n",
            "          ..\n",
            "1599995    1\n",
            "1599996    1\n",
            "1599997    1\n",
            "1599998    1\n",
            "1599999    1\n",
            "Name: target, Length: 1600000, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Encode Sentiments\n",
        "# Map: 0 = negative, 4 = positive\n",
        "df['target'] = df['target'].replace({0:0, 4:1})\n",
        "num_classes = len(df['target'].unique())\n",
        "\n",
        "print(df['target'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s12EgdnwIsef",
        "outputId": "a2a8eea4-79ac-424f-d82d-5da44199da1c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target\n",
            "0    800000\n",
            "1    800000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Split Dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "ofNvMsibIxue"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Tokenize and Pad\n",
        "max_words = 30000   # top 30k words\n",
        "max_len = 40        # tweets are short, so 40 tokens is enough\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "SlQMxC9nI4T5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Build Model (LSTM / GRU)\n",
        "# ---------------------------\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))  # 2 classes\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "vAeVOnRSJGxO",
        "outputId": "28585cc9-4475-48b6-f6c4-c64f65226875"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# 1) Embedding converts each word index → dense vector\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
        "\n",
        "# 2) LSTM now receives 3D input (batch, sequence_length, embedding_dim)\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "\n",
        "# 3) Dropout + Dense\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# 4) Compile\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "oflxfsxcJRGn",
        "outputId": "122de4e7-611b-4967-9241-1fc6170c07d6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(\"Unique labels:\", np.unique(y_train))\n",
        "num_classes = len(np.unique(y_train))\n",
        "print(\"Number of classes:\", num_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vU8jNAkLNNd",
        "outputId": "d996b3a7-ee21-4fd4-d011-8b033ab5e6a0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels: [0 2]\n",
            "Number of classes: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46b371fb"
      },
      "source": [
        "# Re-run train_test_split after correcting target mapping\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text'], df['target'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Value counts for y_train after re-split:\")\n",
        "print(y_train.value_counts())\n",
        "print(\"\\nValue counts for y_test after re-split:\")\n",
        "print(y_test.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98e997a2",
        "outputId": "4bf832d8-95d2-468c-e30b-dcdb5a8c89fc"
      },
      "source": [
        "# Step 8: Train Model\n",
        "history = model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    epochs=3,\n",
        "    batch_size=128,\n",
        "    validation_split=0.1\n",
        ")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1746s\u001b[0m 194ms/step - accuracy: 0.8663 - loss: 0.3085 - val_accuracy: 0.8267 - val_loss: 0.4075\n",
            "Epoch 2/3\n",
            "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1685s\u001b[0m 185ms/step - accuracy: 0.8807 - loss: 0.2776 - val_accuracy: 0.8241 - val_loss: 0.4183\n",
            "Epoch 3/3\n",
            "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1709s\u001b[0m 186ms/step - accuracy: 0.8943 - loss: 0.2487 - val_accuracy: 0.8195 - val_loss: 0.4677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.Ensure 'filtered' contains the text data you want to tokenize\n",
        "filtered = [\"This is a sample sentence to tokenize and pad.\"]\n",
        "\n",
        "tokenize_words = tokenizer.texts_to_sequences(filtered)\n",
        "tokenize_words = pad_sequences(tokenize_words, maxlen=max_len, padding='post', truncating='post')\n",
        "print(tokenize_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08-7z6eU4vRa",
        "outputId": "180df322-0321-4bd0-aab6-13dcff1d64fd"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  27    9    5 6054 5287    3    1    7 4618    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Predict on New Tweet\n",
        "def predict_sentiment(text):\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    pad = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
        "    pred = model.predict(pad)\n",
        "    return pred\n",
        "\n",
        "sample = \"I love this new phone, the battery life is amazing!\"\n",
        "prediction_probabilities = predict_sentiment(sample)\n",
        "print(\"Prediction Probabilities:\", prediction_probabilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlgDLZ6CJngx",
        "outputId": "151519ee-f727-499c-d419-158e4de1c478"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
            "Prediction Probabilities: [[6.6781382e-04 9.9933213e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Predict on New Tweet (cont.)\n",
        "# Assuming the prediction probabilities are in a variable named prediction_probabilities\n",
        "# and the positive class probability is at index 1\n",
        "result = prediction_probabilities[0][1]\n",
        "\n",
        "if result >= 0.7:\n",
        "    print('positive')\n",
        "else:\n",
        "    print('negative')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkwB8SqA2LLe",
        "outputId": "2d62ab07-43eb-40ef-bf75-2799472d9f48"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    }
  ]
}